{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect, time\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import discrepancy, visualization\n",
    "from algorithms import ABC_algorithms, TPABC, SMCABC, SMC2ABC, SNLABC, SNL2ABC\n",
    "from problems import problem_IS\n",
    "\n",
    "import utils_os, utils_math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " sampling from true posterior ... \n",
      "\n",
      "x_obs= [[-1. -1. -1. -1.  1. -1. -1.  1.]\n",
      " [-1.  1. -1. -1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.  1.  1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1.  1.  1. -1. -1. -1. -1. -1.]\n",
      " [ 1.  1.  1. -1. -1. -1. -1.  1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1.  1.]\n",
      " [-1. -1. -1. -1.  1. -1. -1.  1.]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'log_likelihood'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m## Visualize\u001b[39;00m\n\u001b[32m     38\u001b[39m problem.visualize()  \n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m visualization.plot_likelihood(samples=true_samples, log_likelihood_function=\u001b[43mtp_abc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m)\n\u001b[32m     40\u001b[39m plt.savefig(\u001b[33m'\u001b[39m\u001b[33mIS_true_posterior.png\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'log_likelihood'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIUAAACFCAYAAAB12js8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAkpJREFUeJzt3cFq4lAYgNEos1X3Ut//wQrurXszZKAg88GMY1prOudsShcpIfm4hv7cuBrHcRzgyvr6F5iIghAFIQpCFIQoCFEQP4YbXC6X4Xg8DpvNZlitVrccwhOa/iV1Pp+H/X4/rNfreVFMQRwOh488P77Q6+vr8PLyMi+KaYV4/2Pb7Xb4rna73V3HnU6nYSnnen0/Z0Xx/pExBfGdo7jXdmHX5G+PAB40CVEQoiBEQYiCEAUhCkIUhCgIURCi4L7Zxxz3jtq/YufBknY7jHec69vb202DNCsFIQpCFIQoCFEQoiBEQYiCEAUhCkIUhCgIUfD4KemSJo+P3jw9Pum1sVIQoiBEQYiCEAUhCkIUhCgIURCiIERBiIIQBSEKHj86X5JHj7JXT/qecysFIQpCFIQoCFEQoiBEQYiCEAUhCkIUhCgIUTBvSjrnuzAf6Vk37n71eXoNM3cTBSEKQhSEKAhREKIgREGIghAFIQpCFIQoCHtJF2r1iftQrRSEKAhREKIgREGIghAFIQpCFIQoCFEQoiBEwbwp6el0Grbb7b8cwgJZKQhREKIgREGIghAFIQpCFIQoCFEQoiBEQYiCEAVhg/EHbNodF/La51tZKQhREKIgREGIghAFIQpCFIQoCFEQoiBEwX0DsfeBz/S9ltTSrsvfBng3RXE+n3/9PBwOH3NW38xuIV/ie30//3TOq/GGue/lchmOx+Ow2Ww+9Z3QfK7pVk9B7Pf7Yb1ez4uC/4sHTUIUhCgIURCiIERBiILhdz8BD+JoG3JsTHQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DIR = 'results/IS'                                             \n",
    "RERUN = not utils_os.is_file_exist(DIR, 'true_samples.npy') \n",
    "\n",
    "## Define the problem\n",
    "problem = problem_IS.IS_Problem(N=500, n=1)\n",
    "true_theta = problem.get_true_theta()\n",
    "\n",
    "## Get x_o ~ p(x|theta)\n",
    "if RERUN:\n",
    "    # observed data x_o\n",
    "    problem.data_obs = problem.simulator(true_theta)\n",
    "    problem.y_obs = problem.statistics(data=problem.data_obs)\n",
    "    utils_os.save_object(DIR, 'data_obs', problem.data_obs)\n",
    "    utils_os.save_object(DIR, 'y_obs', problem.y_obs)\n",
    "else:\n",
    "    problem.data_obs  = utils_os.load_object(DIR, 'data_obs.npy')\n",
    "    problem.y_obs  = problem.statistics(data=problem.data_obs)\n",
    "    \n",
    "\n",
    "## Get True posterior (rejection sampling approximation with 1D sufficient stat)\n",
    "print('\\n sampling from true posterior ... \\n')\n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.num_sim = 40000\n",
    "hyperparams.num_samples = 150\n",
    "hyperparams.device = 'cuda:3'\n",
    "hyperparams.L = 1\n",
    "tp_abc = TPABC.TP_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n",
    "if RERUN:\n",
    "    tp_abc.run()\n",
    "    true_samples = tp_abc.rej_samples\n",
    "    utils_os.save_object(DIR, 'true_samples', true_samples)\n",
    "else:\n",
    "    tp_abc = utils_os.load_algorithm(DIR, tp_abc)\n",
    "    true_samples = utils_os.load_object(DIR, 'true_samples.npy')\n",
    "    \n",
    "## Visualize\n",
    "problem.visualize()  \n",
    "visualization.plot_likelihood(samples=true_samples, log_likelihood_function=tp_abc.log_likelihood)\n",
    "plt.savefig('IS_true_posterior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMC-ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0\n",
      "# of cpus =  4\n",
      "> learning fake posterior \n",
      "> learning true posterior \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cov must be 2 dimensional and square",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m hyperparams.L = \u001b[32m2\u001b[39m                                 \u001b[38;5;66;03m# number of rounds in sequential learning\u001b[39;00m\n\u001b[32m     10\u001b[39m smc_abc = SMCABC.SMC_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43msmc_abc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m JSD_smc_array = []\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hyperparams.L):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\UFMG\\Mestrado periodo\\BI_PREDEP\\Chen\\neural-approx-ss-lfi-main\\algorithms\\SMCABC.py:174\u001b[39m, in \u001b[36mSMC_ABC.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.sort_samples()\n\u001b[32m    173\u001b[39m \u001b[38;5;28mself\u001b[39m.learn_fake_posterior()\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearn_true_posterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28mself\u001b[39m.prior = \u001b[38;5;28mself\u001b[39m.sample_from_true_posterior   \n\u001b[32m    176\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\UFMG\\Mestrado periodo\\BI_PREDEP\\Chen\\neural-approx-ss-lfi-main\\algorithms\\SMCABC.py:109\u001b[39m, in \u001b[36mSMC_ABC.learn_true_posterior\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m log_weight_array = np.zeros((\u001b[32m40000\u001b[39m))\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m40000\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     theta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfake_posterior\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     pdf_fake_prior = \u001b[38;5;28mself\u001b[39m.pdf_fake_prior(theta)\n\u001b[32m    111\u001b[39m     log_weight_array[i] = -np.log(pdf_fake_prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\UFMG\\Mestrado periodo\\BI_PREDEP\\Chen\\neural-approx-ss-lfi-main\\algorithms\\SMCABC.py:78\u001b[39m, in \u001b[36mSMC_ABC._sample\u001b[39m\u001b[34m(self, distribution)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(distribution) == \u001b[32m2\u001b[39m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# Gaussian\u001b[39;00m\n\u001b[32m     77\u001b[39m     mu, cov = distribution[\u001b[32m0\u001b[39m], distribution[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     theta = \u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal_nd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Copula\u001b[39;00m\n\u001b[32m     81\u001b[39m     copula = distribution[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\UFMG\\Mestrado periodo\\BI_PREDEP\\Chen\\neural-approx-ss-lfi-main\\distributions.py:149\u001b[39m, in \u001b[36mnormal_nd.draw_samples\u001b[39m\u001b[34m(mean, cov, N)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdraw_samples\u001b[39m(mean, cov, N=\u001b[32m1\u001b[39m):\n\u001b[32m    148\u001b[39m     \u001b[38;5;66;03m#return stats.multivariate_normal.rvs(mean, cov, N)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:4222\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: cov must be 2 dimensional and square"
     ]
    }
   ],
   "source": [
    "## Sequential Monte Carlo ABC\n",
    "\n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda'\n",
    "hyperparams.num_sim = 2000                        # number of simulations\n",
    "hyperparams.num_samples = 150                     # number of samples to represent posterior\n",
    "hyperparams.L = 2                                 # number of rounds in sequential learning\n",
    "\n",
    "smc_abc = SMCABC.SMC_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n",
    "smc_abc.run()\n",
    "\n",
    "JSD_smc_array = []\n",
    "for l in range(hyperparams.L):\n",
    "    print('round =', l)\n",
    "    smc_abc.posterior = smc_abc.posterior_array[l]\n",
    "    visualization.plot_likelihood(samples=true_samples, log_likelihood_function=smc_abc.log_likelihood, dimensions=(0,1))\n",
    "    JSD = discrepancy.JSD(tp_abc.log_likelihood, smc_abc.log_likelihood, true_samples, true_samples, N_grid=30)\n",
    "    JSD_smc_array.append(JSD)\n",
    "    print('JSD smc = ', JSD)\n",
    "utils_os.save_object(DIR, 'JSD_SMC', JSD_smc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential Monte Carlo ABC +\n",
    "\n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda'\n",
    "hyperparams.num_sim = 2000                       # number of simulations\n",
    "hyperparams.num_samples = 150                    # number of samples to represent posterior\n",
    "hyperparams.L = 2                                # number of learning rounds\n",
    "hyperparams.type = 'cnn2d'                       # the network architecture of S(x)\n",
    "hyperparams.stat = 'infomax'                     # statistics function: infomax/moment/score  \n",
    "hyperparams.estimator = 'JSD'                    # MI estimator; JSD or DC, see the paper\n",
    "\n",
    "smc2_abc = SMC2ABC.SMC2_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n",
    "smc2_abc.run()\n",
    "\n",
    "JSD_smc2_array = []\n",
    "for l in range(len(smc2_abc.posterior_array)):\n",
    "    print('l=', l)\n",
    "    smc2_abc.l = l\n",
    "    smc2_abc.posterior = smc2_abc.posterior_array[l]\n",
    "    visualization.plot_likelihood(samples=true_samples, log_likelihood_function=smc2_abc.log_likelihood, dimensions=(0,1))\n",
    "    JSD = discrepancy.JSD(tp_abc.log_likelihood, smc2_abc.log_likelihood, true_samples, true_samples, N_grid=30)\n",
    "    JSD_smc2_array.append(JSD)\n",
    "    print('JSD smc2 = ', JSD)\n",
    "utils_os.save_object(DIR, 'JSD_SMC2', JSD_smc2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential Neural Likelihood\n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda'\n",
    "hyperparams.num_sim = 4000\n",
    "hyperparams.L = 2\n",
    "\n",
    "print('\\n SNL ABC')\n",
    "snl_abc = SNLABC.SNL_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n",
    "snl_abc.run()\n",
    "\n",
    "JSD_array = []\n",
    "for l in range(len(snl_abc.nde_array)):\n",
    "    print('l=', l)\n",
    "    snl_abc.nde_net = snl_abc.nde_array[l]\n",
    "    visualization.plot_likelihood(samples=true_samples, log_likelihood_function=snl_abc.log_likelihood, dimensions=(0,1))\n",
    "    JSD = discrepancy.JSD(tp_abc.log_likelihood, snl_abc.log_likelihood, true_samples, true_samples, N_grid=30)\n",
    "    JSD_array.append(JSD)\n",
    "    print('JSD snl = ', JSD)\n",
    "utils_os.save_object(DIR, 'JSD_SNL', JSD_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sequential Neural Likelihood + \n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda'\n",
    "hyperparams.num_sim = 4000                       # number of simulations\n",
    "hyperparams.L = 2                                # number of learning rounds\n",
    "hyperparams.type = 'cnn2d'                       # the network architecture of S(x)\n",
    "hyperparams.stat = 'infomax'                     # statistics function: infomax/moment/score   \n",
    "hyperparams.estimator = 'JSD'                    # MI estimator; JSD or DC, see the paper\n",
    "hyperparams.nde = 'MAF'                          # nde; MAF (D>1) or MDN (D=1)\n",
    "\n",
    "snl2_abc = SNL2ABC.SNL2_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n",
    "snl2_abc.run()\n",
    "\n",
    "JSD_array = []\n",
    "for l in range(len(snl2_abc.nde_array)):\n",
    "    print('l=', l)\n",
    "    snl2_abc.set(l=l)\n",
    "    visualization.plot_likelihood(samples=true_samples, log_likelihood_function=snl2_abc.log_likelihood, dimensions=(0,1))\n",
    "    JSD = discrepancy.JSD(tp_abc.log_likelihood, snl2_abc.log_likelihood, true_samples, true_samples, N_grid=30)\n",
    "    JSD_array.append(JSD)\n",
    "    print('JSD snl+ = ', JSD)\n",
    "utils_os.save_object(DIR, 'JSD_SNL2', JSD_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
